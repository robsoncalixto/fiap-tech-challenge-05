# Research: Report Chat Consultant

**Date**: 2026-02-21 | **Feature**: 006-report-chat-consultant

## R1: OpenRouter Streaming Support

**Decision**: Extend the existing `openrouter.ts` client with a new `callOpenRouterStream()` function that sets `stream: true` in the request body and returns a `ReadableStream`.

**Rationale**: The OpenRouter API (compatible with OpenAI chat completions format) natively supports SSE streaming via `stream: true`. The existing `callOpenRouter()` function uses a plain `fetch` call — adding a streaming variant requires minimal changes: set `stream: true`, return the `response.body` as a `ReadableStream` instead of awaiting `response.json()`. No new dependencies are needed — the Web Streams API is natively available in the Node.js 18+ runtime used by Next.js.

**Alternatives considered**:
- **Vercel AI SDK (`ai` package)**: Provides `streamText()` with built-in OpenAI-compatible provider. Rejected because it adds a significant dependency for a single streaming use case, and the project already has a working OpenRouter client. The raw streaming approach keeps the codebase lean.
- **Server-Sent Events (SSE) library**: Unnecessary — Next.js Route Handlers natively support returning `Response` objects with `ReadableStream` bodies.

## R2: Chat Data Persistence (Supabase)

**Decision**: Two new tables: `chat_conversations` (one-to-one with `reports`) and `chat_messages` (many-to-one with `chat_conversations`). Enforce uniqueness via a `UNIQUE` constraint on `chat_conversations.report_id`.

**Rationale**: The spec requires exactly one chat per report (FR-001) and persistent message history (FR-005). A dedicated `chat_conversations` table provides a clean join point and timestamps. The `chat_messages` table stores individual messages with `role` (user/assistant) and `content`. Row-Level Security (RLS) policies ensure users can only access conversations for their own reports, while the admin client bypasses RLS for shared report views.

**Alternatives considered**:
- **Single table (messages only, with report_id FK)**: Simpler but loses conversation-level metadata (created_at, potential future fields). Adding a conversation entity now avoids a migration later.
- **JSONB column on reports table**: Would avoid new tables but makes querying individual messages, pagination, and RLS policies harder. Poor fit for a growing message history.

## R3: Streaming API Route Pattern

**Decision**: New `POST /api/chat` route that accepts `{ reportId, message }`, validates auth + tier, persists the user message, calls OpenRouter with streaming, and returns the streamed response. After streaming completes, persist the full assistant message.

**Rationale**: Next.js App Router supports returning `new Response(readableStream, { headers: { 'Content-Type': 'text/event-stream' } })` from Route Handlers. The route handles: (1) auth check, (2) tier check (Pro only), (3) fetch/create conversation, (4) persist user message, (5) build context (report markdown + last 20 messages + new user message), (6) stream from OpenRouter, (7) accumulate streamed tokens and persist assistant message on completion.

**Alternatives considered**:
- **Server Action with streaming**: Next.js Server Actions don't natively support streaming responses. Would require workarounds.
- **WebSocket**: Overkill for request-response pattern; adds complexity with no benefit since each message is a discrete request.

## R4: Consultant System Prompt Design

**Decision**: Create a dedicated `consultant-prompt.ts` that exports a function `buildConsultantSystemPrompt(reportMarkdown: string, severitySummary: object)` returning a system message. The prompt instructs the model to act as a security consultant with full knowledge of the report, respond in Portuguese, and be proactive.

**Rationale**: The consultant needs the report content injected into the system prompt so it has full context on every request (FR-004). A function (vs. static string) allows injecting the report-specific content. The proactive initial message (FR-002) is generated by calling the API with no user message — just the system prompt plus a trigger instruction like "Apresente-se e resuma os principais achados do relatório."

**Alternatives considered**:
- **Injecting report as a user message**: Would work but pollutes the conversation history. System prompt injection is cleaner and follows the standard RAG-over-chat pattern.

## R5: Chat Panel UI Architecture

**Decision**: Collapsible side panel component (`ChatPanel`) rendered alongside the report content. On desktop (≥1024px), the report and chat share the viewport in a flex layout. On mobile (<1024px), the chat stacks below. The panel is collapsible via a toggle button.

**Rationale**: Matches the clarification decision (collapsible side panel). The report page currently uses `max-w-5xl mx-auto` — this needs to change to a flex container when the chat is open on desktop. When collapsed, the report returns to its full-width centered layout. The chat panel uses the same design tokens (surface, border, text colors) and typography already defined in `globals.css`.

**Alternatives considered**:
- **Separate route (`/report/[id]/chat`)**: Poor UX — user loses sight of the report while chatting.
- **Modal/overlay**: Blocks report content, defeats the purpose of contextual consultation.

## R6: Gemini Flash Model Selection

**Decision**: Use `google/gemini-2.5-flash` as the model ID for all chat consultant calls via OpenRouter.

**Rationale**: The user explicitly requested "gemini flash". The existing codebase already uses OpenRouter model IDs in this format (e.g., `google/gemini-2.5-flash-lite` for report generation). Gemini Flash offers fast response times and low cost, ideal for a conversational chat feature. The model is NOT gated as pro-only for report generation, but the chat feature itself is Pro-only.

**Alternatives considered**:
- **Gemini 2.5 Flash Lite**: Cheaper but lower quality for conversational responses.
- **User-selectable model**: Over-engineering for an MVP chat consultant; can be added later.

## R7: Initial Proactive Message Generation

**Decision**: When a chat conversation is created for the first time, the system automatically sends a request to the chat API with a predefined trigger message (not visible to the user) that instructs the consultant to introduce itself and summarize key findings. The resulting assistant message becomes the first visible message in the chat.

**Rationale**: FR-002 requires a proactive initial message. Generating it via the same AI pipeline ensures consistency and allows the consultant to reference actual report content. The trigger message is an internal system instruction, not displayed in the UI.

**Alternatives considered**:
- **Static template message**: Would not reference specific report findings; feels generic.
- **Generate on report completion**: Would add latency to report generation and create messages for users who never open the chat.
